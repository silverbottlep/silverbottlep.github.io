<!DOCTYPE html>
<html lang="en">
<head>
  <title>Eunbyung Park</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="css/main.css">
  <link rel="stylesheet" href="css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Nunito">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
</head>

<body>
<nav class="navbar navbar-inverse">
  <div class="container-fluid">
    <div class="navbar-header">
      <a class="navbar-brand" href="index.html">Eunbyung Park</a>
    </div>
    <ul class="nav navbar-nav">
      <li><a href="index.html">Home</a></li>
      <li><a href="https://v-laboratory.github.io/">Team</a></li>
      <li class="active"><a href="publication.html">Publications</a></li>
      <li><a href="teaching.html">Teaching</a></li>
      <li><a href="pic.html">Gallery</a></li>
    </ul>
  </div>
</nav>

  <div id="main">
        <h3>Preprints</h3>
                <li class="list-group-item">
                <p class="paper"><b>Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation</b></p> 
                <p class="paper">Seungjun Oh, Younggeun Lee, Hyejin Jeon, Eunbyung Park</p>
                <p class="paper">arXiv:2505.13215</p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2505.13215">Paper</a>] [<a target="_blank" href="https://ohsngjun.github.io/3D-4DGS/">Project Page</a>] [<a target="_blank" href="https://github.com/ohsngjun/3D-4DGS">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Optimized Minimal 3D Gaussian Splatting</b></p> 
                <p class="paper">Joo Chan Lee, Jong Hwan Ko, Eunbyung Park</p>
                <p class="paper">arXiv:2503.16924</p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2503.16924">Paper</a>] [<a target="_blank" href="https://maincold2.github.io/omg/">Project Page</a>] [<a target="_blank" href="https://github.com/maincold2/OMG">Code</a>]</p>
                </li>            
                <li class="list-group-item">
                <p class="paper"><b>CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting</b></p> 
                <p class="paper">Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park, Eunbyung Park, Sangpil Kim</p>
                <p class="paper">arXiv:2503.12836</p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2503.12836">Paper</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>MetaFormer: High-fidelity Metalens Imaging via Aberration Correcting Transformers</b></p> 
                <p class="paper">Byeonghyeon Lee, Youbin Kim, Yongjae Jo, Hyunsu Kim, Hyemi Park, Yangkyu Kim, Debabrata Mandal, Praneeth Chakravarthula, Inki Kim, Eunbyung Park</p>
                <p class="paper">arXiv:2412.04591</p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2412.04591">Paper</a>] [<a target="_blank" href="https://benhenryl.github.io/MetaFormer/">Project Page</a>] [<a target="_blank" href="https://github.com/benhenryL/MetaFormer">Code</a>]</p>
                </li>
        <h3>2025</h3>
                <li class="list-group-item">
                <p class="paper"><b>Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction</b></p> 
                <p class="paper">Seungtae Nam*, Xiangyu Sun*, Gyeongjin Kang, Younggeun Lee, Seungjun Oh, Eunbyung Park</p>
                <p class="paper">The IEEE/CVF Conference on Computer Vision and Pattern Recognition, <b>CVPR 2025</b></p>
                <p class="paper"><b style="color:#C34A2C">Highlight!</b> (acceptance rate = 3.7%) </p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2412.06234">Paper</a>] [<a target="_blank" href="https://stnamjef.github.io/GenerativeDensification/">Project Page</a>] [<a target="_blank" href="https://github.com/stnamjef/GenerativeDensification">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian Splatting</b></p> 
                <p class="paper">Gyeongjin Kang*, Jisang Yoo*, Jihyeon Park, Seungtae Nam, Hyeonsoo Im, Sangheon Shin, Sangpil Kim, Eunbyung Park</p>
                <p class="paper">The IEEE/CVF Conference on Computer Vision and Pattern Recognition, <b>CVPR 2025</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2411.17190">Paper</a>] [<a target="_blank" href="https://gynjn.github.io/selfsplat/">Project Page</a>] [<a target="_blank" href="https://github.com/Gynjn/selfsplat">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>EditSplat: Multi-View Fusion and Attention-Guided Optimization for View-Consistent 3D Scene Editing with 3D Gaussian Splatting</b></p> 
                <p class="paper">Dong In Lee, Hyeongcheol Park, Jiyoung Seo, Eunbyung Park, Hyunje Park, Ha Dam Baek, Shin Sangheon, Sangmin Kim, Sangpil Kim</p>
                <p class="paper">The IEEE/CVF Conference on Computer Vision and Pattern Recognition, <b>CVPR 2025</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2412.11520">Paper</a>] [<a target="_blank" href="https://kuai-lab.github.io/editsplat2024/">Project Page</a>] [Code (Coming soon)]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Anti-Exposure Bias in Diffusion Models via Prompt Learning</b></p> 
                <p class="paper">Junyu Zhang, Daochang Liu, Eunbyung Park, Shichao Zhang, Chang Xu</p>
                <p class="paper"><p class="paper">International Conference on Learning Representations, <b>ICLR 2025</b></p>
                <p class="paper"><b style="color:#C34A2C">Spotlight!</b> (acceptance rate = 5.1%) </p>
                <p class="paper">[<a target="_blank" href="https://openreview.net/forum?id=MtDd7rWok1">Paper</a>] [Code (Coming soon)]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh Representations</b></p> 
                <p class="paper">Namgyu Kang*, Jaemin Oh*, Youngjoon Hong, Eunbyung Park</p>
                <p class="paper"><p class="paper">International Conference on Learning Representations, <b>ICLR 2025</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2412.05994">Paper</a>] [<a target="_blank" href="https://openreview.net/forum?id=y5B0ca4mjt">OpenReview</a>] [<a target="_blank" href="https://namgyukang.github.io/Physics-Informed-Gaussians/">Project Page</a>] [<a target="_blank" href="https://github.com/NamGyuKang/Physics-Informed-Gaussians">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Separable Physics-informed Neural Networks for Solving the BGK Model of the Boltzmann Equation</b></p> 
                <p class="paper">Jaemin Oh, Seung Yeon Cho, Seok-Bae Yun, Eunbyung Park, Youngjoon Hong</p>
                <p class="paper">SIAM Journal on Scientific Computing, <b>SISC 2025</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2403.06342">Paper</a>]</p>
                </li>                
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Sequence Matters: Harnessing Video Models in 3D Super-Resolution</b></p> 
                <p class="paper">Hyun-kyu Ko*, Dongheok Park*, Youngin Park, Byeonghyeon Lee, Juhee Han, Eunbyung Park</p>
                <p class="paper">The 39th AAAI Conference on Artificial Intelligence, <b>AAAI 2025</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2412.11525">Paper</a>] [<a target="_blank" href="https://ko-lani.github.io/Sequence-Matters/">Project Page</a>] [<a target="_blank" href="https://github.com/DHPark98/SequenceMatters">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>DiffuseHigh: Training-free Progressive High-Resolution Image Synthesis through Structure Guidance</b></p> 
                <p class="paper">Younghyun Kim*, Geunmin Hwang*, Junyu Zhang, Eunbyung Park</p>
                <p class="paper">The 39th AAAI Conference on Artificial Intelligence, <b>AAAI 2025</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2406.18459">Paper</a>] [<a target="_blank" href="https://yhyun225.github.io/DiffuseHigh/">Project Page</a>] [<a target="_blank" href="https://github.com/yhyun225/DiffuseHigh">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis</b></p> 
                <p class="paper">Gyeongjin Kang*, Younggeun Lee*, Seungjun Oh, Eunbyung Park</p>
                <p class="paper">The 39th AAAI Conference on Artificial Intelligence, <b>AAAI 2025</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2404.04913">Paper</a>] [<a target="_blank" href="https://gynjn.github.io/CodecNeRF/">Project Page</a>] [<a target="_blank" href="https://github.com/Gynjn/CodecNeRF">Code</a>]</p>
                </li>
        <h3>2024</h3>
                <li class="list-group-item">
                <p class="paper"><b>Parameter-Efficient Instance-Adaptive Neural Video Compression</b></p> 
                <p class="paper">Seungjun Oh*, Hyunmo Yang, Eunbyung Park</p>
                <p class="paper">The Asian Conference on Computer Vision, <b>ACCV 2024</b></p>
                <p class="paper"><b style="color:#C34A2C">Oral!</b> (acceptance rate = 5.6%) </p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2405.08530">Paper</a>] [<a target="_blank" href="https://github.com/ohsngjun/PEVC">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>F-3DGS: Factorized Coordinates and Representations for 3D Gaussian Splatting</b></p> 
                <p class="paper">Xiangyu Sun, Joo Chan Lee, Daniel Rho, Jong Hwan Ko, Usman Ali, Eunbyung Park</p>
                <p class="paper">The ACM International Conference on Multimedia, <b>ACM MM 2024</b> </p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2405.17083">Paper</a>] [<a target="_blank" href="https://xiangyu1sun.github.io/Factorize-3DGS/">Project Page</a>] [<a target="_blank" href="https://github.com/Xiangyu1Sun/Factorize-3DGS">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Deblurring 3D Gaussian Splatting</b></p> 
                <p class="paper">Byeonghyeon Lee*, Howoong Lee*, Xiangyu Sun, Usman Ali, Eunbyung Park</p>
                <p class="paper">The European Conference on Computer Vision, <b>ECCV 2024</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2401.00834">Paper</a>] [<a target="_blank" href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/">Project Page</a>] [<a target="_blank" href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Continuous Memory Representation for Anomaly Detection</b></p> 
                <p class="paper">Joo Chan Lee*, Taejune Kim*, Eunbyung Park, Simon S. Woo, Jong Hwan Ko</p>
                <p class="paper">The European Conference on Computer Vision, <b>ECCV 2024</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2402.18293">Paper</a>] [<a target="_blank" href="https://tae-mo.github.io/crad/">Project Page</a>] [<a target="_blank" href="https://github.com/tae-mo/GRAD">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Freq-Mip-AA : Frequency Mip Representation for Anti-Aliasing Neural Radiance Fields</b></p> 
                <p class="paper">Youngin Park, Seungtae Nam, Cheul-hee Hahm, Eunbyung Park</p>
                <p class="paper">IEEE International Conference on Image Processing, <b>ICIP 2024</b></p>
                <p class="paper"><b style="color:#C34A2C">Best Paper Candidates!</b> </p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2406.13251">Paper</a>] [<a target="_blank" href="https://github.com/yi0109/FreqMipAA">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Compact 3D Gaussian Representation for Radiance Field</b></p> 
                <p class="paper">Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, Eunbyung Park</p>
                <p class="paper">The IEEE/CVF Conference on Computer Vision and Pattern Recognition, <b>CVPR 2024</b></p>
                <p class="paper"><b style="color:#C34A2C">Highlight!</b> (acceptance rate = 3.6%) </p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2311.13681">Paper</a>] [<a target="_blank" href="https://maincold2.github.io/c3dgs/">Project Page</a>] [<a target="_blank" href="https://github.com/maincold2/Compact-3DGS">Code</a>] [<a target="_blank" href="https://www.youtube.com/watch?v=ooPSDSNikz4">Recorded talk (Korean)</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Residual Learning in Diffusion Models</b></p> 
                <p class="paper">Junyu Zhang, Daochang Liu, Eunbyung Park, Shichao Zhang, Chang Xu</p>
                <p class="paper">The IEEE/CVF Conference on Computer Vision and Pattern Recognition, <b>CVPR 2024</b></p>
                <p class="paper"><b style="color:#C34A2C">Highlight!</b> (acceptance rate = 3.6%) </p>
                <p class="paper">[<a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Residual_Learning_in_Diffusion_Models_CVPR_2024_paper.html">Paper</a>] [<a target="_blank" href="">Code coming soon!</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Coordinate-Aware Modulation for Neural Fields</b></p> 
                <p class="paper">Joo Chan Lee, Daniel Rho, Seungtae Nam, Jong Hwan Ko, Eunbyung Park</p>
                <p class="paper">International Conference on Learning Representations, <b>ICLR 2024</b></p>
                <p class="paper"><b style="color:#C34A2C">Spotlight!</b> (acceptance rate = 6.2%) </p>
                <p class="paper">[<a target="_blank" href="https://openreview.net/forum?id=4UiLqimGm5&noteId=4UiLqimGm5">Paper</a>] [<a target="_blank" href="https://maincold2.github.io/cam/">Project Page</a>] [<a target="_blank" href="https://github.com/maincold2/cam">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Field using Sharpness Prior</b></p> 
                <p class="paper">Byeonghyeon Lee*, Howoong Lee*, Usman Ali, Eunbyung Park</p>
                <p class="paper">The IEEE/CVF Winter Conference on Applications of Computer Vision, <b>WACV 2024</b></p>
                <p class="paper">[<a target="_blank" href="https://openaccess.thecvf.com/content/WACV2024/html/Lee_Sharp-NeRF_Grid-Based_Fast_Deblurring_Neural_Radiance_Fields_Using_Sharpness_Prior_WACV_2024_paper.html">Paper</a>] [<a target="_blank" href="https://benhenryl.github.io/SharpNeRF/">Project Page</a>] [<a target="_blank" href="https://github.com/benhenryL/SharpNeRF">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Rethinking Convolutional Neural Networks for Trajectory Refinement</b></p> 
                <p class="paper">Hanbit Yoon, Usman Ali, Joonhee Choi, Eunbyung Park</p>
                <p class="paper"><b>Pattern Recognition, Elsevier, 2024</b></p>
                <p class="paper">[<a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0031320324006344">Paper</a>] [<a target="_blank" href="">Code coming soon!</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Hydra: Multi-head Low-rank Adaptation for Parameter Efficient Fine-tuning</b></p> 
                <p class="paper">Sanghyeon Kim*, Hyunmo Yang*, Younghyun Kim*, Youngjoon Hong, Eunbyung Park</p>
                <p class="paper"><b>Neural Networks, Elsevier, 2024</b></p>
                <p class="paper">[<a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0893608024003381">Paper</a>] [<a target="_blank" href="https://github.com/extremebird/Hydra">Code</a>]</p>
                </li>
        <h3>2023</h3>
                <li class="list-group-item">
                <p class="paper"><b>Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields</b></p> 
                <p class="paper">Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park</p>
                <p class="paper">Neural Information Processing Systems, <b>NeurIPS 2023</b></p>
                <p class="paper">[<a target="_blank" href="https://openreview.net/pdf?id=BW6nZf7TnK">Paper</a>] [<a target="_blank" href="https://stnamjef.github.io/mipgrid.github.io/">Project Page</a>] [<a target="_blank" href="https://github.com/stnamjef/MipGrid">Code</a>]</p>
                </li> 
                <li class="list-group-item">
                <p class="paper"><b>Separable Physics-Informed Neural Networks</b></p> 
                <p class="paper">Junwoo Cho*, Seungtae Nam*, Hyunmo Yang, Seok-Bae Yun, Youngjoon Hong, Eunbyung Park</p>
                <p class="paper">Neural Information Processing Systems, <b>NeurIPS 2023</b></p>
                <p class="paper"><b style="color:#C34A2C">Spotlight!</b> (acceptance rate = 3.6%) </p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2306.15969">Paper</a>] [<a target="_blank" href="https://jwcho5576.github.io/spinn.github.io/">Project Page</a>] [<a target="_blank" href="https://github.com/stnamjef/SPINN">Code</a>] [<a target="_blank" href="https://arxiv.org/abs/2211.08761">NeurIPS DLDE 2022 Workshop Paper</a>] [<a target="_blank" href="https://youtu.be/S-b26O2OWhI?t=4759">Recorded talk</a>] </p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>FFNeRV: Flow-Guided Frame-Wise Neural Representations for Videos</b></p> 
                <p class="paper">Joo Chan Lee, Daniel Rho, Jong Hwan Ko, Eunbyung Park</p>
                <p class="paper">The ACM International Conference on Multimedia, <b>ACM MM 2023</b> </p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2212.12294">Paper</a>] [<a target="_blank" href="https://maincold2.github.io/ffnerv/">Project Page</a>] [<a target="_blank" href="https://github.com/maincold2/FFNeRV">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Masked Wavelet Representation for Compact Neural Radiance Fields</b></p> 
                <p class="paper">Daniel Rho*, Byeonghyeon Lee*, Seungtae Nam, Joo Chan Lee, Jong Hwan Ko, Eunbyung Park</p>
                <p class="paper">The IEEE/CVF Conference on Computer Vision and Pattern Recognition, <b>CVPR 2023</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2212.09069">Paper</a>] [<a target="_blank" href="https://daniel03c1.github.io/masked_wavelet_nerf/">Project Page</a>] [<a target="_blank" href="https://github.com/daniel03c1/masked_wavelet_nerf">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>SMPConv: Self-Moving Point Representations for Continuous Convolution</b></p> 
                <p class="paper">Sanghyeon Kim, Eunbyung Park</p>
                <p class="paper">The IEEE/CVF Conference on Computer Vision and Pattern Recognition, <b>CVPR 2023</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2304.02330">Paper</a>] [<a target="_blank" href="https://github.com/sangnekim/SMPConv">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>PIXEL: Physics-Informed Cell Representations for Fast and Accurate PDE Solvers</b></p> 
                <p class="paper">Namgyu Kang, Byeonghyeon Lee, Youngjoon Hong, Seok-Bae Yun, Eunbyung Park</p>
                <p class="paper">The 37th AAAI Conference on Artificial Intelligence, <b>AAAI 2023</b></p>
                <p class="paper"><a target="_blank" href="https://dlde-2022.github.io/">The Symbiosis of Deep Learning and Differential Equations</a>, <b>NeurIPS 2022 Workshop</b> <b style="color:#C34A2C">Spotlight!</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2207.12800">Paper</a>] [<a target="_blank" href="https://namgyukang.github.io/PIXEL/">Project Page</a>] [<a target="_blank" href="https://github.com/NamGyuKang/PIXEL">Code</a>]</p>
                </li>
        <h3>2022</h3>
                <li class="list-group-item">
                <p class="paper"><b>Streamable Neural Fields</b></p>
                <p class="paper">Junwoo Cho*, Seungtae Nam*, Daniel Rho, Jong Hwan Ko, Eunbyung Park</p>
                <p class="paper">The European Conference on Computer Vision, <b>ECCV 2022</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2207.09663">Paper</a>] [<a target="_blank" href="https://github.com/jwcho5576/streamable_nf">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Neural Residual Flow Fields for Efficient Video Representations</b></p>
                <p class="paper">Daniel Rho, Junwoo Cho, Jong Hwan Ko, Eunbyung Park</p>
                <p class="paper">The Asian Conference on Computer Vision, <b>ACCV 2022</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/2201.04329">Paper</a>] [<a target="_blank" href="https://github.com/daniel03c1/eff_video_representation">Code</a>]</p>
                </li>
        <h3>Before 2022</h3>
                <li class="list-group-item">
                <p class="paper"><b>Rotationally-Temporally Consistent Novel View Synthesis of Human Performance Video</b></p>
                <p class="paper">YoungJoong Kwon, Stefano Petrangeli, Dahun Kim, Haoliang Wang, Eunbyung Park, Vishy Swaminathan, Henry Fuchs</p>
                <p class="paper">The European Conference on Computer Vision, <b>ECCV 2020</b></p>
                <p class="paper">[<a target="_blank" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490375.pdf">Paper</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Unsupervised Doodling and Painting with Improved SPIRAL</b></p>
                <p class="paper">John F. J. Mellor, Eunbyung Park, Yaroslav Ganin, Igor Babuschkin, Tejas Kulkarni, Dan Rosenbaum, Andy Ballard, Theophane Weber, Oriol Vinyals, S. M. Ali Eslami</p>
                <p class="paper">Machine Learning for Creativity and Design, <b>NeurIPS 2019 Workshop</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/1910.01007">Paper</a>] [<a target="_blank" href="https://learning-to-paint.github.io/">Animated Paper</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Meta-Curvature</b></p>
                <p class="paper">Eunbyung Park, Junier B. Oliva</p>
                <p class="paper">Neural Information Processing Systems, <b>NeurIPS 2019</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/1902.03356">Paper</a>] [<a target="_blank" href="https://github.com/silverbottlep/meta_curvature">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Three years of Low-Power Image Recognition Challenge</b></p>
                <p class="paper">Kent Gauen, Ryan Dailey, Yung-Hsiang Lu, Eunbyung Park, Wei Liu, Alexander C Berg, Yiran Chen </p>
                <p class="paper">Design, Automation and Test in Europe Conference, <b>DATE 2018 Exhibition</b><br>
                <p class="paper">[<a target="_blank" href="https://ieeexplore.ieee.org/document/8342099">Paper</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Meta-Tracker: Fast and Robust Online Adaptation for Visual Object Trackers</b></p>
                <p class="paper">Eunbyung Park, Alexander C. Berg</p>
                <p class="paper">The European Conference on Computer Vision, <b>ECCV 2018</b></p>
                <p class="paper">[<a target="_blank" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Eunbyung_Park_Meta-Tracker_Fast_and_ECCV_2018_paper.pdf">Paper</a>] [<a target="_blank" href="https://github.com/silverbottlep/meta_trackers">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>A Dataset for Developing and Benchmarking Active Vision</b></p>
                <p class="paper">Phil Ammirato, Patrick Poirson, Eunbyung Park, Jana Kosecka, Alexander C. Berg</p>
                <p class="paper">International Conference on Robotics and Automation, <b>ICRA 2017</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/1702.08272">Paper</a>] [<a target="_blank" href="http://www.cs.unc.edu/~ammirato/active_vision_dataset_website/index.html">Project Page</a>]</p> 
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Transformation-Grounded Image Generation Network for Novel 3D View Synthesis</b></p>
                <p class="paper">Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan, Alexander C. Berg</p>
                <p class="paper">The IEEE/CVF Conference on Computer Vision and Pattern Recognition, <b>CVPR 2017</b></p>
                <p class="paper">[<a target="_blank" href="https://arxiv.org/abs/1703.02921">Paper</a>] [<a target="_blank" href="http://www.cs.unc.edu/~eunbyung/tvsn/">Project Page</a>] [<a target="_blank" href="https://github.com/silverbottlep/tvsn">Code</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>An Evaluation of the NVIDIA TX1 for Supporting Real-Time Computer Vision Workloads</b></p>
                <p class="paper">Nathan Otterness, Ming Yang, Sarah Rust, Eunbyung Park, James H. Anderson, F. Donelson Smith, Alexander C. Berg, Shige Wang</p>
                <p class="paper">Real-Time and Embedded Technology and Applications Symposium, <b>RTAS 2017</b></p>
                <p class="paper">[<a target="_blank" href="https://cs.unc.edu/~anderson/papers/rtas17b.pdf">Paper</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Registration of Pathological Images</b></p>
                <p class="paper">Xiao Yang, Xu Han, Eunbyung Park, Stephen Aylward, Roland Kwitt, Marc Niethammer</p>
                <p class="paper">Simulation an Synthesis in Medical Imaging, <b>MICCAI 2016 Workshop</b></p>
                <p class="paper">[<a target="_blank" href="http://wwwx.cs.unc.edu/~mn/sites/default/files/yang_miccai_2016_tumor.pdf">Paper</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Learning to Decompose for Object Detection and Instance Segmentation</b></p>
                <p class="paper">Eunbyung Park, Alexander C. Berg</p>
                <p class="paper">International Conference on Learning Representations, <b>ICLR 2016 Workshop</b></p>
                <p class="paper">[<a target="_blank" href="http://arxiv.org/abs/1511.06449">Paper</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Combining Multiple Sources of Knowledge in Deep CNNs for Action Recognition</b></p>
                <p class="paper">Eunbyung Park, Xufeng Han, Tamara L. Berg, Alexander C. Berg</p>
                <p class="paper">IEEE Winter Conference on Applications of Computer Vision, <b>WACV 2016</b></p>
                <p class="paper">[<a target="_blank" href="https://www.cs.unc.edu/~eunbyung/papers/wacv2016_combining.pdf">Paper</a>]</p>
                </li>
                <li class="list-group-item">
                <p class="paper"><b>Visual Madlibs: Fill-in-the-blank Image Description and Question Answering</b></p>
                <p class="paper">Licheng Yu, Eunbyung Park, Alexander C. Berg, Tamara L. Berg</p>
                <p class="paper">International Conference on Computer Vision, <b>ICCV 2015</b></p>
                <p class="paper"> <div class="description"> [<a target="_blank" href="https://www.cs.unc.edu/~eunbyung/papers/iccv15_madlibs.pdf">Paper</a>][<a target="_blank" href="http://tamaraberg.com/visualmadlibs/">Project Page</a>][<a target="_blank" href="https://www.cs.unc.edu/~eunbyung/images/iccv15_madlibs.mp4">Spotlight Video</a>][<a target="_blank" href="https://www.cs.unc.edu/~eunbyung/papers/iccv15_madlibs_supp.pdf">Supplementary File</a>] </p>
                </li>

  </div>
</body>
</html>
